{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37e8b402"
      },
      "source": [
        "### Research Assistant Agent Class\n",
        "\n",
        "This code defines the `ResearchAssistantAgent` class, which simulates a research assistant. It's designed to:\n",
        "\n",
        "- **Process user queries**: Extract the topic and scope of research.\n",
        "- **Search the web**: Retrieve information from online sources using the `googlesearch-python` library.\n",
        "- **Evaluate source credibility**: Assign a simple credibility score based on the URL and content length.\n",
        "- **Summarize content**: Generate a brief summary of each retrieved source.\n",
        "- **Generate a report**: Create a Word document (`.docx`) containing the research findings and citations using the `python-docx` library.\n",
        "- **Apply reinforcement learning**: Update source selection weights based on simulated user feedback (though the feedback mechanism here is a simple example).\n",
        "\n",
        "**Key Methods:**\n",
        "\n",
        "- `__init__()`: Initializes the agent, including its memory system to store sources, summaries, citations, feedback logs, and source weights.\n",
        "- `process_input(query)`: Parses the user's research query.\n",
        "- `evaluate_source_credibility(url, content)`: Calculates a credibility score for a given source.\n",
        "- `search_web(query)`: Performs the web search and retrieves content from the top results.\n",
        "- `summarize_content(content)`: Summarizes the text content of a source.\n",
        "- `extract_keywords(content)`: Extracts keywords from the source content.\n",
        "- `generate_report(query, sources)`: Creates the Word document report.\n",
        "- `apply_reinforcement_learning(action, reward)`: Updates internal weights based on feedback (simulated).\n",
        "- `run(query, user_feedback)`: The main method to execute the research process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48a9c215"
      },
      "source": [
        "### Library Installation and Imports\n",
        "\n",
        "This code cell installs the necessary Python libraries and imports the required modules for the `ResearchAssistantAgent`.\n",
        "\n",
        "- `!pip install requests beautifulsoup4 googlesearch-python`: Installs libraries for making HTTP requests, parsing HTML, and performing Google searches.\n",
        "- `!pip install python-docx`: Installs the library for creating and modifying Word documents.\n",
        "- `!pip install numpy`: Installs the NumPy library, commonly used for numerical operations (though not extensively used in this specific agent implementation).\n",
        "\n",
        "The subsequent import statements make the functions and classes from these libraries available for use in the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbTEB_rcRkZT",
        "outputId": "47b0c270-4463-4105-c4bb-9d10bee429f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Collecting googlesearch-python\n",
            "  Downloading googlesearch_python-1.3.0-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.7.14)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.14.1)\n",
            "Downloading googlesearch_python-1.3.0-py3-none-any.whl (5.6 kB)\n",
            "Installing collected packages: googlesearch-python\n",
            "Successfully installed googlesearch-python-1.3.0\n",
            "Collecting python-docx\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.14.1)\n",
            "Downloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx\n",
            "Successfully installed python-docx-1.2.0\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n"
          ]
        }
      ],
      "source": [
        "# Install necessary libraries\n",
        "!pip install requests beautifulsoup4 googlesearch-python\n",
        "!pip install python-docx\n",
        "!pip install numpy\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from googlesearch import search\n",
        "import numpy as np\n",
        "from docx import Document\n",
        "import re\n",
        "from typing import List, Dict, Tuple\n",
        "import json\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ResearchAssistantAgent:\n",
        "    def __init__(self):\n",
        "        # Memory system: Store search results, summaries, and citations\n",
        "        self.memory = {\n",
        "            'sources': [],  # List of (url, content, credibility_score)\n",
        "            'summaries': [],  # List of summaries\n",
        "            'citations': [],  # List of citation entries\n",
        "            'feedback_log': [],  # List of (action, reward) for RL\n",
        "            'source_weights': {}  # Weights for source selection (RL policy)\n",
        "        }\n",
        "        self.max_sources = 5  # Limit number of sources to manage resources\n",
        "\n",
        "    def process_input(self, query: str) -> Tuple[str, Dict]:\n",
        "        \"\"\"Parse user query to extract topic and scope.\"\"\"\n",
        "        # Basic input validation\n",
        "        if not query or len(query.strip()) < 3:\n",
        "            raise ValueError(\"Invalid query: Query must be at least 3 characters long.\")\n",
        "        if any(word in query.lower() for word in ['inappropriate', 'harmful']):\n",
        "            raise ValueError(\"Query contains inappropriate content.\")\n",
        "\n",
        "        # Extract topic and scope (simplified parsing)\n",
        "        scope = {'max_results': self.max_sources, 'topic': query.strip()}\n",
        "        return query, scope\n",
        "\n",
        "    def evaluate_source_credibility(self, url: str, content: str) -> float:\n",
        "        \"\"\"Evaluate source credibility based on domain and content quality.\"\"\"\n",
        "        # Simplified credibility scoring\n",
        "        credibility = 0.5  # Baseline score\n",
        "        if 'edu' in url or 'gov' in url:\n",
        "            credibility += 0.3\n",
        "        if len(content) > 500:  # Longer content often indicates depth\n",
        "            credibility += 0.2\n",
        "        return min(credibility, 1.0)\n",
        "\n",
        "    def search_web(self, query: str) -> List[Dict]:\n",
        "        \"\"\"Perform web search and retrieve content.\"\"\"\n",
        "        sources = []\n",
        "        try:\n",
        "            for url in search(query, num_results=self.max_sources):\n",
        "                try:\n",
        "                    response = requests.get(url, timeout=5)\n",
        "                    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "                    content = ' '.join([p.text for p in soup.find_all('p')])\n",
        "                    credibility = self.evaluate_source_credibility(url, content)\n",
        "                    sources.append({\n",
        "                        'url': url,\n",
        "                        'content': content[:1000],  # Limit content size\n",
        "                        'credibility': credibility\n",
        "                    })\n",
        "                    # Update source weights for RL\n",
        "                    self.memory['source_weights'][url] = self.memory.get('source_weights', {}).get(url, 0.5) + credibility * 0.1\n",
        "                except Exception as e:\n",
        "                    print(f\"Error fetching {url}: {str(e)}\")\n",
        "                    continue\n",
        "        except Exception as e:\n",
        "            print(f\"Search error: {str(e)}\")\n",
        "            # Fallback strategy: Return cached sources if available\n",
        "            if self.memory['sources']:\n",
        "                return self.memory['sources']\n",
        "        return sources\n",
        "\n",
        "    def summarize_content(self, content: str) -> str:\n",
        "        \"\"\"Summarize content using simple truncation and keyword extraction.\"\"\"\n",
        "        sentences = re.split(r'[.!?]+', content)\n",
        "        keywords = self.extract_keywords(content)\n",
        "        summary = ' '.join(sentences[:2])  # Take first two sentences\n",
        "        return f\"{summary} (Keywords: {', '.join(keywords)})\"\n",
        "\n",
        "    def extract_keywords(self, content: str) -> List[str]:\n",
        "        \"\"\"Extract top keywords from content.\"\"\"\n",
        "        words = re.findall(r'\\w+', content.lower())\n",
        "        word_freq = {}\n",
        "        for word in words:\n",
        "            if len(word) > 3:  # Ignore short words\n",
        "                word_freq[word] = word_freq.get(word, 0) + 1\n",
        "        return sorted(word_freq, key=word_freq.get, reverse=True)[:3]\n",
        "\n",
        "    def generate_report(self, query: str, sources: List[Dict]) -> str:\n",
        "        \"\"\"Generate a Word document with findings and citations.\"\"\"\n",
        "        doc = Document()\n",
        "        doc.add_heading(f\"Research Report: {query}\", 0)\n",
        "\n",
        "        for i, source in enumerate(sources, 1):\n",
        "            doc.add_heading(f\"Source {i}: {source['url']}\", level=1)\n",
        "            summary = self.summarize_content(source['content'])\n",
        "            doc.add_paragraph(f\"Summary: {summary}\")\n",
        "            doc.add_paragraph(f\"Credibility Score: {source['credibility']:.2f}\")\n",
        "            self.memory['citations'].append(f\"{i}. {source['url']}\")\n",
        "\n",
        "        doc.add_heading(\"Citations\", level=1)\n",
        "        for citation in self.memory['citations']:\n",
        "            doc.add_paragraph(citation)\n",
        "\n",
        "        output_file = \"research_report.docx\"\n",
        "        doc.save(output_file)\n",
        "        return output_file\n",
        "\n",
        "    def apply_reinforcement_learning(self, action: str, reward: float):\n",
        "        \"\"\"Update policy based on feedback.\"\"\"\n",
        "        self.memory['feedback_log'].append((action, reward))\n",
        "        # Update source weights based on average reward\n",
        "        if action.startswith(\"select_source_\"):\n",
        "            url = action[len(\"select_source_\"):]\n",
        "            current_weight = self.memory['source_weights'].get(url, 0.5)\n",
        "            self.memory['source_weights'][url] = current_weight + reward * 0.1\n",
        "\n",
        "    def run(self, query: str, user_feedback: float = None):\n",
        "        \"\"\"Main execution loop.\"\"\"\n",
        "        try:\n",
        "            # Process input\n",
        "            query, scope = self.process_input(query)\n",
        "\n",
        "            # Retrieve and process sources\n",
        "            sources = self.search_web(query)\n",
        "            if not sources:\n",
        "                return \"No sources found. Please try a different query.\"\n",
        "\n",
        "            self.memory['sources'] = sources\n",
        "            for source in sources:\n",
        "                self.memory['summaries'].append(self.summarize_content(source['content']))\n",
        "\n",
        "            # Generate report\n",
        "            report_file = self.generate_report(query, sources)\n",
        "\n",
        "            # Apply RL if feedback provided\n",
        "            if user_feedback is not None:\n",
        "                for source in sources:\n",
        "                    self.apply_reinforcement_learning(f\"select_source_{source['url']}\", user_feedback)\n",
        "\n",
        "            return f\"Report generated: {report_file}\"\n",
        "        except Exception as e:\n",
        "            print(f\"Error in agent execution: {str(e)}\")\n",
        "            return \"An error occurred. Please try again.\""
      ],
      "metadata": {
        "id": "_DqX1d6WR5SN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfae8882"
      },
      "source": [
        "### Example Usage\n",
        "\n",
        "This code cell demonstrates how to use the `ResearchAssistantAgent` class.\n",
        "\n",
        "- `agent = ResearchAssistantAgent()`: Creates an instance of the `ResearchAssistantAgent`.\n",
        "- `result = agent.run(\"artificial intelligence ethics\")`: Runs the agent with the query \"artificial intelligence ethics\". This will trigger the web search, summarization, report generation, and simulated reinforcement learning.\n",
        "- `print(result)`: Prints the result of the `run` method, which is a message indicating the report file name.\n",
        "- `agent.apply_reinforcement_learning(\"select_source_example.com\", 1.0)`: This line simulates user feedback. In a real-world scenario, this would likely come from user interaction with the generated report. The feedback (1.0 in this case, representing positive feedback) is used to update the agent's internal source weights, influencing future searches."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "agent = ResearchAssistantAgent()\n",
        "result = agent.run(\"artificial intelligence ethics\")\n",
        "print(result)\n",
        "\n",
        "# Simulate user feedback (1.0 = positive, -1.0 = negative)\n",
        "agent.apply_reinforcement_learning(\"select_source_example.com\", 1.0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0m42wXlCSPwU",
        "outputId": "1c4f1e29-2f6a-407b-9d21-5f5b32021d88"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Report generated: research_report.docx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Safety and Security Measures\n",
        "\"\"\"\n",
        "## Safety and Security Measures\n",
        "- **Input Validation**: Checks for query length and inappropriate content.\n",
        "- **Boundary Enforcement**: Limits number of sources to 5 to prevent overuse.\n",
        "- **Fallback Strategies**: Uses cached sources if web search fails.\n",
        "- **Transparency**: Reports errors and limitations in output.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "zPz06vNxSVRd"
      }
    }
  ]
}